{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems\n",
    "\n",
    "## Matrix Computing \n",
    "\n",
    "Physical systems are often modeled by systems of simultaneous equations written in matrix form.  Realistic models correspond to rather larger matrices, where it is important to use a good linear algebra library.  Computers are unusually good with matrix manipulations because those manipulations typically involve simple instructions that can be iterate many times and algorithms exist to do this quite efficiently.  Subroutines for matrix computing are found in well-established scientific libraries (e.g., `scipy` and `numpy`), where these subroutines are usually \n",
    "\n",
    "- ${\\sim}10\\times$ faster (or more) than the elementary methods found in linear algebra textbooks,\n",
    "- designed to minimize the round-off error, and \n",
    "- have a high chance of success for a broad class of problems.\n",
    "\n",
    "For these reasons, you *should not write your own matrix methods* (unless absolutely necessary), but instead get them from a library.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes of Matrix Problems\n",
    "\n",
    "There are some rules of mathematics that help you understand problems when solving equations.  For example, you should encounter problems if:\n",
    "\n",
    "- you have more unknowns than equations, or\n",
    "- if your equations are not linearly independent.\n",
    "\n",
    "While you cannot obtain a unique solution when there are not enough equations, you may be able to *map out* a space of allowable solutions.  \n",
    "\n",
    "If you have more equations than unknowns, then you have an *overdetermined* problem, which may not have a unique solution.  An overdetermined problem is sometimes treated using data fitting techniques in which a solution to a sufficient set of equations is found, tested on the unused equations, and then improved as necessary.   This technique is calle the *linear least squares method* because the method minimizes the disagreement with the equations.\n",
    "\n",
    "The most basic matrix problem is a system of linear equations:\n",
    "\n",
    "```{math}\n",
    ":label: linear_eqn\n",
    "\\mathbf{A}\\mathbf{x} = \\mathbf{b},\n",
    "```\n",
    "\n",
    "which is defined by a known $N\\times N$ matrix $\\mathbf{A}$, an unknown vector $\\mathbf{x}$ of length $N$, and a known vector $\\mathbf{b}$ of length $N$.  The obvious way to solve this equation is to determine the inverse of $\\mathbf{A}$ (i.e., $\\mathbf{A}^{-1}$) and then multiply both sides to get:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}^{-1}\\mathbf{A} &=  I_N,\\qquad \\text{(Identity property)}\\\\\n",
    "\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}, \\\\\n",
    "\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}.\n",
    "\\end{align}\n",
    "\n",
    "Both the direct solution of Eq. {eq}`linear_eqn` and the process of matrix inversion are standard in a matrix subroutine library.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "src=\"https://www.youtube.com/embed/uQhTuRlWMxw\"\n",
    "frameborder=\"0\" \n",
    "allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \n",
    "allowfullscreen></iframe>\n",
    "\n",
    "</div>\n",
    "\n",
    "```{note}\n",
    "A more efficient way to solve Eq. {eq}`linear_eqn` is by Gaussian elimination or [lower-upper (LU) decomposition](https://www.quantstart.com/articles/LU-Decomposition-in-Python-and-NumPy/) because it yields the vector $\\mathbf{x}$ without explicitly calculating $\\mathbf{A}^{-1}$.  Sometimes, you may want the inverse for other purposes, such that the method of multiplying by the inverse is preferred.\n",
    "```\n",
    "<div align=\"center\">\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "src=\"https://www.youtube.com/embed/ZDxONtacA_4\"\n",
    "frameborder=\"0\" \n",
    "allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \n",
    "allowfullscreen></iframe>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Other matrix problems are of the following form:\n",
    "\n",
    "```{math}\n",
    ":label: eigenvalue_eqn\n",
    "\\mathbf{A}\\mathbf{x} = \\lambda \\mathbf{x},\n",
    "```\n",
    "\n",
    "which is similar to Eq. {eq}`linear_eqn`, but with an unknown parameter $\\lambda$.  This form is called the *eigenvalue* problem.  It is harder to solve because solutions exist for only certain (if any) values of $\\lambda$.  To find a solution, we use the identity matrix to rewrite it as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}x - \\lambda\\mathbf{x} = 0, \\\\\n",
    "[\\mathbf{A}-\\lambda I_N ]\\mathbf{x} = 0.\n",
    "\\end{align}\n",
    "\n",
    "Multiplying by $[\\mathbf{A}-\\lambda I_N ]^{-1}$ yields the *trivial* solution $\\mathbf{x} = 0$.  A more interesting solution implies the nonexistence of the inverse.  The inverse fails to exist when the determinant is zero, or\n",
    "\n",
    "\\begin{align}\n",
    "\\det{[\\mathbf{A}-\\lambda I_N]} = 0.\n",
    "\\end{align}\n",
    "\n",
    "The values of $\\lambda$ that satisfy this *secular* equation are the eigenvalues of Eq. {eq}`eigenvalue_eqn`.  To solve this equation, you need a subroutine to calculate the determinant of a matrix, and then a search routine to find the zero (root).\n",
    "\n",
    "The traditional way to solve Eq. {eq}`eigenvalue_eqn` for both eigenvalues and eigenvectors is by *diagonalization*.  This is a process where a sequence of transformations (using a matrix $\\mathbf{U}$) are continually operating on the original equation until one is found so that $\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1} = \\lambda I_N$.  Mathematically, this is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{U}\\mathbf{A}(\\mathbf{U}^{-1}\\mathbf{U})\\mathbf{x} &= \\lambda \\mathbf{U}\\mathbf{x},\\\\\n",
    "(\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1})(\\mathbf{U}\\mathbf{x}) &= \\lambda \\mathbf{U}\\mathbf{x}, \\\\\n",
    "\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1} &= \\begin{pmatrix}\n",
    "\\lambda_1^\\prime & & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2^\\prime & \\cdots & 0 \\\\\n",
    "0 & 0 & \\lambda_3^\\prime& \\cdots \\\\\n",
    "0 & \\cdots & & \\lambda_N^\\prime \n",
    "\\end{pmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "The diagonal values of $\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1}$ are the eigenvalues with the eigenvectors\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}_i = \\mathbf{U}^{-1}\\hat{e}_i.\n",
    "\\end{align}\n",
    "\n",
    "The eigenvectors are just the columns of the matrix $\\mathbf{U}^{-1}$, where there are a number of routines of this type found in subroutine libraries.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<iframe width=\"560\" height=\"315\"\n",
    "src=\"https://www.youtube.com/embed/PFDu9oVAE-g\"\n",
    "frameborder=\"0\" \n",
    "allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" \n",
    "allowfullscreen></iframe>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Matrix Computing\n",
    "\n",
    "Many scientific programming bugs arise from the improper use of arrays (e.g., `IndexError`).  Matrices are extensively used in scientific computing, where it can be difficult to keep track of maximum index or matrix dimensions.  Here are some things to consider:\n",
    "\n",
    "- **Computers are finite**: Large matrices require more memory, which can slow down your computation significantly.  For example, a 4D array containing 100 elements in each dimension can have $(100)^4$ $64-\\text{byte}$ words that occupy $\\simeq 1\\ {\\rm GB}$ of memory.\n",
    "- **Processing time**: Matrix operations (e.g., inversion) require ${\\sim}N^3$ steps for a square matrix of dimension $N$.  Doubling the dimensions of a 2D square matrix leads to an *eightfold* increase in processing time.\n",
    "- **Paging**: When a program runs out of RAM (i.e., fast memory), the operating system can use disk space instead and is called *paging*.  The process of writing to the disk is a slow process, which you want to avoid.  If your program is near the memory limit at which paging occurs, even a slight increase in a matrix's dimensions may lead to a *tenfold* increase in execution time.\n",
    "- **Matrix storage**:  Matrices can be represented as a multidimensional block of stored numbers that appear in a linear order.  A $3\\times 3$ matrix can be stored in\n",
    "\n",
    "  - **row-major order**: $[a_{11},a_{12},a_{13},a_{21},a_{22},a_{23},a_{31},a_{32},a_{33}]$ (in Java or python), or\n",
    "  - **column-major order**: $[a_{11},a_{21},a_{31},a_{12},a_{22},a_{32},a_{13},a_{23},a_{33}]$ (in Fortran).\n",
    "\n",
    "    The element $a_{ij}$ is represented by the $i\\text{th}$ row and $j\\text{th}$ column.  The row-major order traverses *across each row* (i.e., increases the column index $j$) before advancing to the next row (i.e., increases the row index $i$).  The column-major order traverses *down each column* before advancing to the next column.\n",
    "- **Subscript 0**: In python, C, and Java the indexing of arrays begin with the value 0.  Fortran has recently modified its standard to match these other languages, but older codes will have a standard to start the indices at 1.  \n",
    "  \n",
    "  Let `a = np.zeros((3,3))` represent a $3\\times 3$ matrix (in python) initialized with $0$ for each element, where $a[0,0] = a_{11}$.  In Fortran, this element would be represented as $a(1,1) = a_{11}$.\n",
    "- **Physical and logical dimensions**:  Some programming languages require you to issue commands (e.g., `double a[3][3]` or `zeros((3,3), Float))`) that tell the compiler how much memory it needs to set aside fo the array `a`.  This is called *physical memory*, where a matrix's *logical size* is the amount of memory that your actually use to store numbers.\n",
    "\n",
    "  Modern programming languages permit *dynamic memory allocation*, where you may use variables as the dimension of your arrays and read in the values of the variables at run time (i.e., after the program is compiled).  As a result the matrices will have the same physical and logical sizes.\n",
    "\n",
    "  However, `Fortran77` requires the dimensions to be specified at *compile time*, which can allow the physical and logical sizes to differ.  As a result, some subroutines from a library may need to know both the physical and logical sizes of your arrays.\n",
    "- **Passing sizes to subprograms**: This is needed when the logical and physical dimensions of arrays differ.  When using external libraries, you must watch that the sizes of your matrices do not exceed the bound that have been declared in the subprograms.  This may occur *without* an error message and can give you the *wrong answers*.  If you are running a python program that calls a Fortran subroutine, you will need to pass *pointers* (i.e., addresses in memory) to variables and not the actual values of the variables.\n",
    "- **Equivalence, pointers, references manipulations**: Computers used to have much more limited amounts of memory (see the [Apollo Guidance Computer](https://en.wikipedia.org/wiki/Apollo_Guidance_Computer)), which forced programmers to conserve memory by having different variables occupy the *same* memory location.  This is not a problem as long as these variables were not being used at the same time.  Fortran does this through the `Common` and `Equivalence` statements, where other languages can acheive this through manipulations of pointers and references. However, this is not a good practice as the complexity of the program grows significantly.  Do not use them unless it is a matter of \"life or death\"!\n",
    "- **Say what's happening**: You decrease programming errors by using self-explanatory labels for your indices (subscripts), and comments (i.e., stating what your variables mean and describing your storage schemes).\n",
    "- **Tests**: Always test a library routine on a small problem that your already known the answer to.  Then you'll know if you are supplying it with the right arguments and have all the links working. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra in `numpy`\n",
    "Python does not create what we normally think of as abstract matrices when we use 2D arrays. Fortunately there is the `LinearAlgebra` package that treats 2D arrays (a 1D array of 1D arrays) as abstract matrices, and also provides a simple interface.\n",
    "\n",
    "Consider the standard matrix equation (Eq. {eq}`linear_eqn`), where $\\mathbf{A}$ is a $3\\times 3$ matrix, $\\mathbf{b}$ is a $3 \\times 1$ vector, and the program will figure out the dimensions of vector $\\mathbf{x}$.  In python, we start with the `import` packages and verify our input with some `print` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  [[  1   2   3]\n",
      " [ 22  32  42]\n",
      " [ 55  66 100]]\n",
      "b =  [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "A = np.array([[1,2,3],[22,32,42],[55,66,100]]) # Array of arrays\n",
    "print(\"A = \", A)\n",
    "b = np.array([1,2,3])\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now solve $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ using `solve` from `numpy.linalg`, and test how close $\\mathbf{A}\\mathbf{x}-\\mathbf{b}$ comes to a zero vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [-1.4057971  -0.1884058   0.92753623]\n",
      "Residual =  [ 2.22044605e-16  2.66453526e-15 -1.77635684e-15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import solve\n",
    "\n",
    "x = np.linalg.solve(A,b) #Does solution\n",
    "print(\"x = \", x)\n",
    "print(\"Residual = \", np.dot(A,x)-b ) #LHS - RHS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solved the entire set of linear equations (by elimination) with just the single command `solve`, performed a matrix multiplication (i.e., dot product) with `np.dot`, did a matrix subtraction, and obtained a solution close to machine precision.\n",
    "\n",
    "A more direct way of solving Eq. {eq}`linear_eqn` is by calculating the inverse $\\mathbf{A}^{-1}$, and then finding the dot product of the RHS with the inverse, $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A^{-1}A =  [[ 1.00000000e+00 -4.30211422e-16 -1.08246745e-15]\n",
      " [-1.94289029e-16  1.00000000e+00 -9.99200722e-16]\n",
      " [ 9.71445147e-17  4.71844785e-16  1.00000000e+00]]\n",
      "x =  [-1.4057971  -0.1884058   0.92753623]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import inv\n",
    "\n",
    "A_inv = inv(A)\n",
    "#test inverse through identity property\n",
    "print(\"A^{-1}A = \", np.dot(A_inv,A))\n",
    "#find solution using the inverse\n",
    "print(\"x = \", np.dot(A_inv,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested that `inv(A)` was true through the identity property, where $\\mathbf{A}^{-1}\\mathbf{A} = I_3$.  The result was accurate for the diagonal elements and near machine precision for the off-diagonal elements.  Then we used the inverse to solve the matrix equation directly, and found the same answer as before, within machine precision.\n",
    "\n",
    "Now, consider solving the eigenvalue equation (Eq. {eq}`eigenvalue_eqn`) as the principal-axes system for a cube (i.e., rotational motion).  This requires us to determine a coordinate system in which the **inertia tensor** is diagonal, or\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I}\\omega = \\lambda \\omega,\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{I}$ represents the inertia matrix, $\\omega$ is an eigenvector, and $\\lambda$ is an eigenvalue.  This can be solved using the `eig` function from `numpy.linalg` given the inertia matrix as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{I} = \\begin{pmatrix}\n",
    "\\frac{2}{3} & -\\frac{1}{4} & -\\frac{1}{4} \\\\\n",
    "-\\frac{1}{4} & \\frac{2}{3} & -\\frac{1}{4} \\\\\n",
    "-\\frac{1}{4} & -\\frac{1}{4} & \\frac{2}{3}\n",
    "\\end{pmatrix}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I =  [[ 0.66666667 -0.25       -0.25      ]\n",
      " [-0.25        0.66666667 -0.25      ]\n",
      " [-0.25       -0.25        0.66666667]]\n",
      "Eigenvalues =  [0.91666667 0.16666667 0.91666667]\n",
      "Matrix of Eigenvectors =  [[ 0.81649658 -0.57735027  0.43514263]\n",
      " [-0.40824829 -0.57735027 -0.81589244]\n",
      " [-0.40824829 -0.57735027  0.38074981]]\n",
      "--------------------\n",
      "LHS - RHS =  [ 1.11022302e-16 -5.55111512e-17 -1.11022302e-16]\n",
      "LHS =  [ 0.7484552 -0.3742276 -0.3742276]\n",
      "RHS =  [ 0.7484552 -0.3742276 -0.3742276]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig \n",
    "\n",
    "I = np.array([[2./3.,-0.25,-0.25],[-0.25,2./3.,-0.25],[-0.25,-0.25,2./3.]])\n",
    "print(\"I = \", I) #checking that I was input correctly\n",
    "\n",
    "Es, evectors = eig(I)\n",
    "print(\"Eigenvalues = \", Es)\n",
    "print(\"Matrix of Eigenvectors = \", evectors) #Transpose is incorrectly printed in the textbook\n",
    "\n",
    "print(\"--------------------\")\n",
    "Vec = evectors[:,0] # a single eigenvector to test RHS = LHS\n",
    "LHS = np.dot(I,Vec)\n",
    "RHS = np.dot(Vec,Es[0])\n",
    "\n",
    "print(\"LHS - RHS = \", LHS-RHS)\n",
    "print(\"LHS = \", LHS)\n",
    "print(\"RHS = \", RHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Matrix Programs\n",
    "Here are some exercises to see how to solve matrix problems in general.\n",
    "\n",
    "1. Find the numerical inverse of $\\mathbf{A} = \\begin{pmatrix} 4 & -2 & 1 \\\\ 3 & 6 & -4 \\\\ 2 & 1 & 8 \\end{pmatrix}$.\n",
    "\n",
    "Note that the analytical solution can be determined using the `sympy` module, which is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A^{-1} = '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{52}{263} & \\frac{17}{263} & \\frac{2}{263}\\\\- \\frac{32}{263} & \\frac{30}{263} & \\frac{19}{263}\\\\- \\frac{9}{263} & - \\frac{8}{263} & \\frac{30}{263}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "⎡  52      17         ⎤\n",
       "⎢ ───     ───    2/263⎥\n",
       "⎢ 263     263         ⎥\n",
       "⎢                     ⎥\n",
       "⎢ -32      30      19 ⎥\n",
       "⎢ ────    ───     ─── ⎥\n",
       "⎢ 263     263     263 ⎥\n",
       "⎢                     ⎥\n",
       "⎢                  30 ⎥\n",
       "⎢-9/263  -8/263   ─── ⎥\n",
       "⎣                 263 ⎦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sympy as sym\n",
    "from IPython.display import display\n",
    "sym.init_printing()\n",
    "\n",
    "A = sym.Matrix([[4, -2, 1], [3, 6, -4],[2, 1, 8]])\n",
    "display(\"A^{-1} = \",A.inv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  [[ 4 -2  1]\n",
      " [ 3  6 -4]\n",
      " [ 2  1  8]]\n",
      "-------\n",
      "A^{-1} =  [[ 0.19771863  0.06463878  0.00760456]\n",
      " [-0.121673    0.11406844  0.07224335]\n",
      " [-0.03422053 -0.03041825  0.11406844]]\n",
      "-------\n",
      "A^{-1}A =  [[ 1.00000000e+00 -3.46944695e-18  5.55111512e-17]\n",
      " [ 2.77555756e-17  1.00000000e+00  2.22044605e-16]\n",
      " [ 2.77555756e-17  0.00000000e+00  1.00000000e+00]]\n",
      "-------\n",
      "error in A^{-1} using analytical result =  [[ 0.00000000e+00  0.00000000e+00 -6.07153217e-18]\n",
      " [ 0.00000000e+00  1.38777878e-17 -2.77555756e-17]\n",
      " [ 0.00000000e+00  3.46944695e-18 -1.38777878e-17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import inv \n",
    "\n",
    "A = np.array([[4, -2, 1], [3, 6, -4],[2, 1, 8]])\n",
    "inv_A = inv(A)\n",
    "print(\"A = \", A)\n",
    "print(\"-------\")\n",
    "print(\"A^{-1} = \", inv_A)\n",
    "print(\"-------\")\n",
    "print(\"A^{-1}A = \", np.dot(inv_A,A))\n",
    "print(\"-------\")\n",
    "analytical_inv_A = (1./263.)*np.array([[52, 17, 2],[-32, 30, 19], [-9,-8, 30]])\n",
    "error = analytical_inv_A - inv_A\n",
    "print(\"error in A^{-1} using analytical result = \", error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider the same matrix $\\mathbf{A}$ as before, but now used to describe three simultaneous linear equations, $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.  Here, $\\mathbf{b}$ is known and has three component vectors:\n",
    "   \\begin{align}\n",
    "   b_1 = \\begin{pmatrix} 12 \\\\ 4 \\\\ 20 \\end{pmatrix} &\\quad b_2 = \\begin{pmatrix} -25 \\\\ -10 \\\\ -30 \\end{pmatrix} & b_3 = \\begin{pmatrix} 32 \\\\ 22 \\\\ 40 \\end{pmatrix}.\n",
    "   \\end{align}\n",
    "\n",
    "   Determine the three different $\\mathbf{x}$ vectors for each component vector in $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 =  [1, -2, 4]\n",
      "x_2 =  [0.312, -0.038, 2.677]\n",
      "x_3 =  [2.319, -2.966, 4.791]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import solve \n",
    "\n",
    "A = np.array([[4, -2, 1], [3, 6, -4],[2, 1, 8]])\n",
    "b = np.array([[12, 4, 20], [-25, -10, -30], [32, 22, 40]])\n",
    "x = solve(A,b)\n",
    "\n",
    "for i in range(0,3):\n",
    "    if i ==0:\n",
    "        print(\"x_%i = \" % (i+1), \"[%i, %i, %i]\" % (x[0,i],x[1,i],x[2,i]))\n",
    "    else:\n",
    "        print(\"x_%i = \" % (i+1), \"[%1.3f, %1.3f, %1.3f]\" % (x[0,i],x[1,i],x[2,i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider the matrix $\\mathbf{A} = \\begin{pmatrix} \\alpha & \\beta \\\\ -\\beta & \\alpha \\end{pmatrix}$, where you are free to use any values you want for $\\alpha$ and $\\beta$.  Show that the eigenvalues and eigenvectors are the complex conjugates $x_{1,2}$ and $\\lambda_{1,2}$, respectively. Note that python uses $j$ to denote a complex number instead of $i$.\n",
    "   \n",
    "\\begin{align} \n",
    "x_{1,2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} +1 \\\\ \\pm i \\end{pmatrix}, &\\quad \\lambda_{1,2} = \\alpha \\mp i\\beta.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0.587  beta =  1.321\n",
      "sqrt(2)*x_1 =  [1.+0.j 0.+1.j]\n",
      "lambda_1 =  (0.587+1.321j)\n",
      "sqrt(2)*x_2 =  [1.+0.j 0.-1.j]\n",
      "lambda_2 =  (0.587-1.321j)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import eig \n",
    "\n",
    "alpha = np.around(np.random.rand(),4)\n",
    "beta = 10*np.around(np.random.rand(),4)\n",
    "A = np.array([[alpha, beta], [-beta, alpha]])\n",
    "Es, evectors = eig(A)\n",
    "\n",
    "print(\"alpha = \",alpha, \" beta = \", beta)\n",
    "for i in range(0,2):\n",
    "    print(\"sqrt(2)*x_%i = \" % (i+1), np.sqrt(2)*evectors[:,i])\n",
    "    print(\"lambda_%i = \" % (i+1), np.around(Es[i],4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Eigenvalues in Arbitrary Potential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nucleon in a box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues via ODE Sovler + Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerov Algorithm for Sch&ouml;dinger ODE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "fe1676e9af49d5a18eb59ea9b6c2a47994bca5ebac1b02ce1831a8d7518265f3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
