{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Eigenvalue Problems\n",
                "\n",
                "## Matrix Computing \n",
                "\n",
                "Physical systems are often modled by systems of simultaneous equations written in matrix form.  Realistic models correspond to rather larger matrices, where it is important to use a good linear algebra library.  Computers are unusually good with matrix manipulations because those manipulations typically involve simple instructrions that can be iterate many times and algorithms exist to do this quite efficiently.  Subroutines for matrix copmuting are found in well-established scientific libraries (e.g., `scipy` and `numpy`), where these subroutines are usually \n",
                "\n",
                "- ${\\sim}10\\times$ faster (or more) than the elementary methods found in linear algebra textbooks,\n",
                "- designed to minimize the round-off error, and \n",
                "- have a high chance of success for a broad class of problems.\n",
                "\n",
                "For these reasons, you *should not write your own matrix methods* (unless absolutely necessary), but instead get them from a library.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Classes of Matrix Problems\n",
                "\n",
                "There are some rules of mathematics that help you understand problems when solving equations.  For example, you should encounter problems if:\n",
                "\n",
                "- you have more unknowns than equations, or\n",
                "- if your equations are not linearly independent.\n",
                "\n",
                "While you cannot obtain a unique solution when there are not enough equations, you may be able to *map out* a space of allowable solutions.  \n",
                "\n",
                "If you have more equations than unknowns, then you have an *overdetermined* problem, which may not have a unique solution.  An overdetermined problem is sometimes treated using data fitting techniques in which a solution to a sufficient set of equations is found, tested on the unused equations, and then improved as necessary.   This technique is calle the *linear least squares method* because the method minimizes the disagreement with the equations.\n",
                "\n",
                "The most basic matrix problem is a system of linear equations:\n",
                "\n",
                "```{math}\n",
                ":label: linear_eqn\n",
                "\\mathbf{A}\\mathbf{x} = \\mathbf{b},\n",
                "```\n",
                "\n",
                "which is defined by a known $N\\times N$ matrix $\\mathbf{A}$, an unknown vector $\\mathbf{x}$ of length $N$, and a known vector $\\mathbf{b}$ of length $N$.  The obvious way to solve this equation is to determine the inverse of $\\mathbf{A}$ (i.e., $\\mathbf{A}^{-1}$) and then multipy both sides to get:\n",
                "\n",
                "\\begin{align}\n",
                "\\mathbf{A}^{-1}\\mathbf{A} &=  I_N,\\qquad \\text{(Identity property)}\\\\\n",
                "\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}, \\\\\n",
                "\\mathbf{x} &= \\mathbf{A}^{-1}\\mathbf{b}.\n",
                "\\end{align}\n",
                "\n",
                "Both the direct solution of Eq. {eq}`linear_eqn` and the process of matrix inversion are standard in a matrix subroutine library.\n",
                "\n",
                "```{note}\n",
                "A more efficient way to solve Eq. {eq}`linear_eqn` is by Gaussian elimination or lower-upper (LU) decomposition because it yields the vector $\\mathbf{x}$ without explicitly calculating $\\mathbf{A}^{-1}$.  Sometimes, you may want the inverse for other purposes, such that the method of multiplying by the inverse is preferred.\n",
                "```\n",
                "\n",
                "Other matrix problems are of the following form:\n",
                "\n",
                "```{math}\n",
                ":label: eigenvalue_eqn\n",
                "\\mathbf{A}\\mathbf{x} = \\lambda \\mathbf{x},\n",
                "```\n",
                "\n",
                "which is similar to Eq. {eq}`linear_eqn`, but with an unknown parameter $\\lambda$.  This form is called the *eigenvalue* problem.  It is harder to solve because solutions exist for only certain (if any) values of $\\lambda$.  To find a solution, we use the identity matrix to rewrite it as:\n",
                "\n",
                "\\begin{align}\n",
                "\\mathbf{A}x - \\lambda\\mathbf{x} = 0, \\\\\n",
                "[\\mathbf{A}-\\lambda I_N ]\\mathbf{x} = 0.\n",
                "\\end{align}\n",
                "\n",
                "Multiplying by $[\\mathbf{A}-\\lambda I_N ]^{-1}$ yields the *trivial* solution $\\mathbf{x} = 0$.  A more interesting solution implies the nonexistence of the inverse.  The inverse fails to exist when the determinant is zero, or\n",
                "\n",
                "\\begin{align}\n",
                "\\det{[\\mathbf{A}-\\lambda I_N]} = 0.\n",
                "\\end{align}\n",
                "\n",
                "The values of $\\lambda$ that satisfy this *secular* equation are the eigenvalues of Eq. {eq}`eigenvalue_eqn`.  To solve this equation, you need a subroutine to calculate the determinat of a matrix, and then a search routine to find the zero (root).\n",
                "\n",
                "The traditional way to solve Eq. {eq}`eigenvalue_eqn` for both eigenvalues and eigenvectors is by *diagonalization*.  This is a process where a sequence of transformations (using a matrix $\\mathbf{U}$) are continually operating on the original equation until one is found so that $\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1} = \\lambda I_N$.  Mathematically, this is:\n",
                "\n",
                "\\begin{align}\n",
                "\\mathbf{U}\\mathbf{A}(\\mathbf{U}^{-1}\\mathbf{U})\\mathbf{x} &= \\lambda \\mathbf{U}\\mathbf{x},\\\\\n",
                "(\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1})(\\mathbf{U}\\mathbf{x}) &= \\lambda \\mathbf{U}\\mathbf{x}, \\\\\n",
                "\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1} &= \\begin{pmatrix}\n",
                "\\lambda_1^\\prime & & \\cdots & 0 \\\\\n",
                "0 & \\lambda_2^\\prime & \\cdots & 0 \\\\\n",
                "0 & 0 & \\lambda_3^\\prime& \\cdots \\\\\n",
                "0 & \\cdots & & \\lambda_N^\\prime \n",
                "\\end{pmatrix}.\n",
                "\\end{align}\n",
                "\n",
                "The diagnoal values of $\\mathbf{U}\\mathbf{A}\\mathbf{U}^{-1}$ are the eignvalues with the eigenvectors\n",
                "\n",
                "\\begin{align}\n",
                "\\mathbf{x}_i = \\mathbf{U}^{-1}\\hat{e}_i.\n",
                "\\end{align}\n",
                "\n",
                "The eigenvectors are just the columns of the matrix $\\mathbf{U}^{-1}$, where ther are a number of rountines of this type found in subroutine libraries.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Practical Matrix Computing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Linear Algebra in `numpy`"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing Matrix Programs"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quantum Eigenvalues in Arbitrary Potential"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Nucleon in a box"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Eigenvalues via ODE Sovler + Search"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Numerov Algorithm for Sch&Ouml;dinger ODE"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "fe1676e9af49d5a18eb59ea9b6c2a47994bca5ebac1b02ce1831a8d7518265f3"
        },
        "kernelspec": {
            "display_name": "Python 3.8.8 64-bit ('base': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
